---
---

@inproceedings{kim_demystifying_2024,
  title = {Demystifying SGD with Doubly Stochastic Gradients},
  author = {Kim, Kyurae and Ko, Joohwan and Ma, Yi-An and Gardner, Jacob R},
  year = {2024},
  month = July,
  selected = {true},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning} (ICML) (to be presented)}},
  teaser = {/assets/img/teasers/doublysgd.png},
  link = {https://arxiv.org/abs/2406.00920}
}

@inproceedings{ko_provably_2024,
  title = {Provably Scalable Black-Box Variational Inference with Structured Variational Families},
  author = {Ko, Joohwan and Kim, Kyurae and Kim, Woo Chang and Gardner, Jacob R},
  year = {2024},
  month = July,
  selected = {true},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning} (ICML) (to be presented)}},
  teaser = {/assets/img/teasers/bbvi_structured.png},
  link = {https://arxiv.org/abs/2401.10989}
}

@inproceedings{kim_saem_2024,
  title = {Stochastic Approximation with Biased {{MCMC}} for Expectation-Maximization},
  author = {Gruffaz, Samuel and Kim, Kyurae and Durmus, Alain and Gardner, Jacob R},
  year = {2024},
  month = May,
  selected = {true},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Machine Learning} (AISTATS)}},
  teaser = {/assets/img/teasers/saem.png},
  link = {https://arxiv.org/abs/2402.17870}
}

@inproceedings{kim_stl_2024,
  title = {Linear Convergence of Black-Box Variational Inference: Should We Stick the Landing?},
  author = {Kim, Kyurae and Ma, Yi-An and Gardner, Jacob R.},
  year = {2024},
  month = May,
  selected = {true},
  booktitle = {Proceedings of the {{International Conference}} on {{Artificial Intelligence}} and {{Machine Learning} (AISTATS)}},
  link = {https://arxiv.org/abs/2307.14642}
}

@inproceedings{kim_convergence_2023,
  title = {On the Convergence of Black-Box Variational Inference},
  author = {Kim, Kyurae and Oh, Jisu and Wu, Kaiwen and Ma, Yi-An and Gardner, Jacob R.},
  year = {2023},
  month = December,
  selected = {true},
  booktitle = {{{Advances}} in {{Neural}} {{Information}} {{Processing}} {{Systems}} (to be presented)},
  teaser = {/assets/img/teasers/bbvi_convergence.png},
  link = {https://arxiv.org/abs/2305.15349}
}

@inproceedings{kim_behavior_2023,
  title = {The Behavior and Convergence of Local {{Bayesian}} Optimization},
  author = {Wu, Kaiwen and Kim, Kyurae and Garnett, Roman and Gardner, Jacob R.},
  year = {2023},
  month = December,
  selected = {true},
  booktitle = {{{Advances}} in {{Neural}} {{Information}} {{Processing}} {{Systems}} (to be presented)},
  teaser = {/assets/img/teasers/local_bo.png},
  link = {https://arxiv.org/abs/2305.15572}
}

@article{kim_automatic_2023,
  title = {Automatic calculation of myocardial perfusion reserve using deep learning with uncertainty quantification},
  author = {Kim, Yoon-Chul and Kim, Kyurae and Choe, Yeon Hyeon},
  year = {2023},
  month = October,
  %pages = {1--1},
  journal = {Quantitative Imaging in Medicine and Surgery},
  selected = {false},
}

@inproceedings{kim_practical_2023,
  title = {Practical and Matching Gradient Variance Bounds for Black-Box Variational {{Bayesian}} Inference},
  author = {Kim, Kyurae and Wu, Kaiwen and Oh, Jisu and Gardner, Jacob R.},
  year = {2023},
  month = July,
  selected = {true},
  booktitle = {Proceedings of the {{International Conference}} on {{Machine Learning} (ICML)}},
  teaser = {/assets/img/teasers/bbvi_gradient_variance.png},
  link = {https://proceedings.mlr.press/v202/kim23w.html}
}

@inproceedings{kim_markov_2022,
  title = {{{Markov}} {{Chain}} {{Score}} {{Ascent}}: {{A}} Unifying Framework of Variational Inference with {{Markovian}} Gradients},
  author = {Kim, Kyurae and Oh, Jisu and Gardner, Jacob R. and Dieng, Adji Bousso and Kim, Hongseok},
  year = {2022},
  selected = {true},
  teaser = {/assets/img/teasers/mcsa_airfoil.png},
  booktitle = {{{Advances}} in {{Neural}} {{Information}} {{Processing}} {{Systems}}},
  code = {https://github.com/Red-Portal/KLpqVI.jl},
  link = {https://proceedings.neurips.cc/paper_files/paper/2022/hash/e0fbc0f2e35e58aeffe5524a69ba90e5-Abstract-Conference.html}
}

@article{kim_probabilistic_2020,
  title = {A Probabilistic Machine Learning Approach to Scheduling Parallel Loops with {{Bayesian}} Optimization},
  author = {Kim, Kyurae and Kim, Youngjae and Park, Sungyong},
  year = {2020},
  pages = {1--1},
  journal = {IEEE Transactions on Parallel and Distributed Systems},
  selected = {true},
  teaser = {/assets/img/teasers/bofss_system.png},
  code = {https://github.com/Red-Portal/bosched},
  link = {https://www.researchgate.net/publication/344295850_A_Probabilistic_Machine_Learning_Approach_to_Scheduling_Parallel_Loops_with_Bayesian_Optimization}
}

@article{kim_automatic_2020,
  title = {Automatic Myocardial Segmentation in Dynamic Contrast Enhanced Perfusion {{MRI}} Using {{Monte Carlo}} Dropout in an Encoder-Decoder Convolutional Neural Network},
  author = {Kim, Yoon-Chul and Kim, Kyurae and Choe, Yeon Hyeon},
  year = {2020},
  volume = {185},
  pages = {105150},
  journal = {Computer Methods and Programs in Biomedicine},
  language = {en},
  selected = {false},
  teaser = {/assets/img/teasers/perfusion_mri_mc.png},
  link = {https://www.researchgate.net/publication/336731977_Automatic_myocardial_segmentation_in_dynamic_contrast_enhanced_perfusion_MRI_using_Monte_Carlo_dropout_in_an_encoder-decoder_convolutional_neural_network}
}

@article{kim_evcmr_2019,
  title={{{EVCMR}}: {{A}} Tool for the Quantitative Evaluation and Visualization of Cardiac {{MRI}} Data},
  shorttitle={{{EVCMR}}},
  author={Kim, Yoon-Chul and Kim, Kyurae and Choi, Kwanghee and Kim, Minwoo and Chung, Younjoon and Choe, Yeon Hyeon},
  year={2019},
  volume={111},
  pages={103334},
  journal={Computers in Biology and Medicine},
  language={en},
  teaser = {/assets/img/teasers/cardiac_analysis.png},
  link = {https://www.researchgate.net/publication/333869321_EVCMR_A_tool_for_the_quantitative_evaluation_and_visualization_of_cardiac_MRI_data}
}

@article{kim_fast_2021,
  title = {Fast Calculation Software for Modified {{Look}}-{{Locker}} Inversion Recovery ({{MOLLI}}) {{T1}} Mapping},
  author = {Kim, Yoon-Chul and Kim, Kyurae and Lee, Hyelee and Choe, Yeon Hyeon},
  year = {2021},
  volume = {21},
  pages = {26},
  abstract = {Abstract                            Background               The purpose of this study was to develop a software tool and evaluate different T1 map calculation methods in terms of computation time in cardiac magnetic resonance imaging.                                         Methods               The modified Look-Locker inversion recovery (MOLLI) sequence was used to acquire multiple inversion time (TI) images for pre- and post-contrast T1 mapping. The T1 map calculation involved pixel-wise curve fitting based on the T1 relaxation model. A variety of methods were evaluated using data from 30 subjects for computational efficiency: MRmap, python Levenberg\textendash Marquardt (LM), python reduced-dimension (RD) non-linear least square, C++ single- and multi-core LM, and C++ single- and multi-core RD.                                         Results               Median (interquartile range) computation time was 126~s (98\textendash 141) for the publicly available software MRmap, 261~s (249\textendash 282) for python LM, 77~s (74\textendash 80) for python RD, 3.4~s (3.1\textendash 3.6) for C++ multi-core LM, and 1.9~s (1.9\textendash 2.0) for C++ multi-core RD. The fastest C++ multi-core RD and the publicly available MRmap showed good agreement of myocardial T1 values, resulting in 95\% Bland\textendash Altman limits of agreement of (-\,0.83 to 0.58~ms) and (-\,6.57 to 7.36~ms) with mean differences of -\,0.13~ms and 0.39~ms, for the pre- and post-contrast, respectively.                                         Conclusion               The C++ multi-core RD was the fastest method on a regular eight-core personal computer for pre- or post-contrast T1 map calculation. The presented software tool (fT1fit) facilitated rapid T1 map and extracellular volume fraction map calculations.},
  file = {/home/msca8h/Zotero/storage/H54N7HXH/Kim et al. - 2021 - Fast calculation software for modified Look-Locker.pdf},
  journal = {BMC Medical Imaging},
  language = {en},
  number = {1},
  teaser = {/assets/img/teasers/t1_mapping.png},
  link = {https://www.researchgate.net/publication/349267167_Fast_calculation_software_for_modified_Look-Locker_inversion_recovery_MOLLI_T1_mapping}
}

@inproceedings{kim_robust_2019b,
  title = {Towards Robust Data-Driven Parallel Loop Scheduling Using {{Bayesian}} Optimization},
  booktitle = {Proceedings of the {{IEEE International Symposium}} on {{Modeling}}, {{Analysis}}, and {{Simulation}} of {{Computer}} and {{Telecommunication Systems}}},
  author = {Kim, Kyurae and Kim, Youngjae and Park, Sungyong},
  year = {2019},
  pages = {241--248},
  address = {{Rennes, FR}},
  code = {https://github.com/Red-Portal/bosched},
  link = {https://www.researchgate.net/publication/334881528_Towards_Robust_Data-Driven_Parallel_Loop_Scheduling_Using_Bayesian_Optimization}
}

@article{son_gpu_2021,
  title = {A GPU Scheduling Framework to Accelerate Hyper-Parameter Optimization in Deep Learning Clusters},
  author = {Jaewon Son and Yonghyuk Yoo and Kyurae Kim and Youngjae Kim and Kwonyong Lee and Sungyong Park},
  year = {2021},
  volume = {10},
  pages = {350},
  abstract = {This paper proposes Hermes, a container-based preemptive GPU scheduling framework for accelerating hyper-parameter optimization in deep learning (DL) clusters. Hermes accelerates hyper-parameter optimization by time-sharing between DL jobs and prioritizing jobs with more promising hyper-parameter combinations. Hermes's scheduling policy is grounded on the observation that good hyper-parameter combinations converge quickly in the early phases of training. By giving higher priority to fast-converging containers, Hermes's GPU preemption mechanism can accelerate training. This enables users to find optimal hyper-parameters faster without losing the progress of a container. We have implemented Hermes over Kubernetes and compared its performance against existing scheduling frameworks. Experiments show that Hermes reduces the time for hyper-parameter optimization up to 4.04 times against previously proposed scheduling policies such as FIFO, round-robin (RR), and SLAQ, with minimal time-sharing overhead.},
  file = {/home/msca8h/Zotero/storage/SCZ8HI74/Son et al. - 2021 - A GPU Scheduling Framework to Accelerate Hyper-Par.pdf},
  journal = {Electronics},
  language = {en},
  number = {3},
  teaser = {/assets/img/teasers/hermes.png},
  link = {https://www.researchgate.net/publication/348980534_A_GPU_Scheduling_Framework_to_Accelerate_Hyper-Parameter_Optimization_in_Deep_Learning_Clusters}
}

